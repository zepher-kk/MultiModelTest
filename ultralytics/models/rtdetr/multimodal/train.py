# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
RT-DETR MultiModal trainer module.

This module provides the RTDETRMMTrainer class for training multi-modal RT-DETR models
with support for RGB+X modality inputs.
"""

import torch
from copy import copy
from pathlib import Path
from typing import Optional

from ultralytics.models.yolo.detect.train import DetectionTrainer
from ultralytics.models.rtdetr.val import RTDETRValidator
from ultralytics.data.build import build_yolo_dataset, build_dataloader
from ultralytics.data.dataset import YOLOMultiModalImageDataset
from ultralytics.nn.tasks import RTDETRDetectionModel
from ultralytics.utils import LOGGER, DEFAULT_CFG, RANK, colorstr
from ultralytics.utils.torch_utils import de_parallel
from ultralytics.utils.patches import torch_load


class RTDETRMMTrainer(DetectionTrainer):
    """
    A trainer class for RT-DETR MultiModal (RTDETRMM) object detection models.

    This class extends DetectionTrainer to support multi-modal inputs (RGB + X modality)
    during the training process. It integrates seamlessly with YOLOMM's data pipeline
    while maintaining RT-DETR's training strategies.

    Key Features:
        - Supports RGB+X dual-modal training with 6-channel input
        - Automatic detection of multi-modal configuration (-mm suffix)
        - Integration with YOLOMultiModalImageDataset
        - Support for single-modal ablation training via modality parameter
        - Graceful degradation to standard RT-DETR when multi-modal unavailable

    Attributes:
        args: Training arguments and hyperparameters.
        model: The RTDETRMM model being trained.
        modality: Specific modality for single-modal training (None for dual-modal).
        is_multimodal: Whether the model is configured for multi-modal operation.
        multimodal_config: Configuration for multi-modal training.

    Methods:
        get_dataset: Build multi-modal dataset using YOLOMultiModalImageDataset.
        get_model: Initialize RT-DETR model with multi-modal support.
        preprocess_batch: Preprocess batch data for multi-modal inputs.
        get_validator: Return multi-modal compatible validator.

    Examples:
        >>> # Dual-modal training
        >>> trainer = RTDETRMMTrainer(overrides={'model': 'rtdetr-r18-mm.yaml', 'data': 'multimodal-dataset.yaml'})
        >>> trainer.train()
        
        >>> # Single-modal ablation
        >>> trainer = RTDETRMMTrainer(overrides={'model': 'rtdetr-r18-mm.yaml', 'modality': 'thermal'})
        >>> trainer.train()
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        Initialize RTDETRMMTrainer for multi-modal training.

        Args:
            cfg (str | DictConfig, optional): Configuration file path or configuration dictionary.
            overrides (dict, optional): Dictionary to override default training arguments.
            _callbacks (list, optional): List of callback functions.
        """
        if overrides is None:
            overrides = {}
        overrides["task"] = "detect"  # Ensure task type is correct
        
        # Pre-initialize multi-modal attributes before parent init
        # This is necessary because parent init calls get_dataset
        model_name = str(overrides.get('model', cfg.get('model', '')))
        self.is_multimodal = '-mm' in Path(model_name).stem
        self.modality = overrides.get('modality', None)
        self.is_dual_modal = self.modality is None
        self.is_single_modal = self.modality is not None
        self.multimodal_config = None
        
        # Initialize parent trainer
        super().__init__(cfg, overrides, _callbacks)
        
        # Update modality from args after parent init
        self.modality = getattr(self.args, 'modality', None)
        self.is_dual_modal = self.modality is None
        self.is_single_modal = self.modality is not None
        
        # Log initialization with modality information
        if self.is_multimodal:
            if self.modality:
                LOGGER.info(f"Initializing RTDETRMMTrainer - Single-modal training mode: {self.modality}-only")
            else:
                LOGGER.info("Initializing RTDETRMMTrainer - Dual-modal training mode (RGB+X)")
        else:
            LOGGER.info("Initializing RTDETRMMTrainer - Standard RT-DETR mode")
    
    def _check_multimodal_model(self):
        """
        Check if the model is configured for multi-modal operation.
        
        Returns:
            bool: True if model name contains '-mm' suffix
        """
        model_name = str(self.args.model)
        return '-mm' in Path(model_name).stem
    
    def build_dataset(self, img_path, mode="train", batch=None):
        """
        Build multi-modal dataset using YOLOMultiModalImageDataset.

        This method follows YOLOMM's successful pattern by overriding build_dataset
        instead of get_dataset, ensuring proper initialization timing and data access.

        Args:
            img_path (str): Path to images
            mode (str): Dataset mode ('train', 'val', 'test')
            batch (int, optional): Batch size for rectangle training

        Returns:
            Dataset: YOLOMultiModalImageDataset for multi-modal, standard dataset otherwise
        """
        if not self.is_multimodal:
            raise ValueError(
                "RTDETRMMTrainer is designed for multi-modal RT-DETR models only. "
                "For standard RT-DETR training, use RTDETRTrainer instead. "
                "Multi-modal models should have '-mm' in the model name."
            )

        # Get model stride parameter (consistent with DetectionTrainer)
        gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)

        # Lazy loading: parse multi-modal configuration on demand
        if not hasattr(self, 'multimodal_config') or self.multimodal_config is None:
            self.multimodal_config = self._parse_multimodal_config()
            LOGGER.info(f"å¤šæ¨¡æ€é…ç½®è§£æå®Œæˆ - æ¨¡æ€: {self.multimodal_config['models']}")

        # Use parsed modality configuration
        modalities = self.multimodal_config['models']
        
        # ğŸ”‘ å…³é”®ä¿®å¤ï¼šæ˜ç¡®æå–Xæ¨¡æ€ä¿¡æ¯ï¼ˆä¸YOLOMMä¿æŒä¸€è‡´ï¼‰
        x_modality = [m for m in self.multimodal_config['models'] if m != 'rgb'][0]
        x_modality_dir = self.multimodal_config['modalities'][x_modality]

        LOGGER.info(f"æ„å»ºå¤šæ¨¡æ€æ•°æ®é›† - æ¨¡å¼: {mode}, è·¯å¾„: {img_path}, æ¨¡æ€: {modalities}")

        # If single-modal training is enabled, log modality padding info and validate compatibility
        if self.modality:
            self._validate_modality_compatibility()
            LOGGER.info(f"å¯ç”¨å•æ¨¡æ€è®­ç»ƒ: {self.modality}-onlyï¼Œå°†åº”ç”¨æ™ºèƒ½æ¨¡æ€å¡«å……")

        # Call build_yolo_dataset with multi_modal_image=True to enable multi-modal dataset
        return build_yolo_dataset(
            self.args, img_path, batch, self.data,
            mode=mode,
            rect=mode == "val",  # âœ… ä¿®å¤ï¼šç»Ÿä¸€rectå‚æ•°å¤„ç†é€»è¾‘
            stride=gs,
            multi_modal_image=True,  # Key parameter: enable YOLOMultiModalImageDataset
            x_modality=x_modality,  # âœ… ä¿®å¤ï¼šä¼ é€’æ¨¡æ€ç±»å‹
            x_modality_dir=x_modality_dir,  # âœ… ä¿®å¤ï¼šä¼ é€’æ¨¡æ€ç›®å½•
            enable_self_modal_generation=getattr(self.args, 'enable_self_modal_generation', False)  # âœ… ä¿®å¤ï¼šæ·»åŠ è‡ªåŠ¨æ¨¡æ€ç”Ÿæˆæ”¯æŒ
        )
    
    
    def get_model(self, cfg: Optional[dict] = None, weights: Optional[str] = None, verbose: bool = True):
        """
        Initialize and return RT-DETR model with multi-modal support.
        
        This method creates RTDETRDetectionModel with correct channel configuration,
        following the same pattern as YOLOMM for consistency.
        
        Args:
            cfg (dict, optional): Model configuration.
            weights (str, optional): Path to pre-trained model weights.
            verbose (bool): Verbose logging if True.
            
        Returns:
            RTDETRDetectionModel: Initialized model ready for multi-modal routing.
        """
        from ultralytics.utils import RANK
        
        # è®¡ç®—æ­£ç¡®çš„è¾“å…¥é€šé“æ•° (å‚è€ƒYOLOMMçš„é€»è¾‘)
        if self.is_dual_modal:
            # åŒæ¨¡æ€è®­ç»ƒï¼šä»dataé…ç½®ä¸­è¯»å–Xch
            x_channels = self.data.get('Xch', 3)
            channels = 3 + x_channels  # RGB(3) + X(Xch)
            if verbose and RANK in {-1, 0}:
                LOGGER.info(f"å¤šæ¨¡æ€RT-DETRæ¨¡å‹åˆå§‹åŒ–: RGB(3ch) + X({x_channels}ch) = {channels}chæ€»è¾“å…¥")
        else:
            # å•æ¨¡æ€è®­ç»ƒï¼šå§‹ç»ˆä½¿ç”¨3é€šé“
            channels = 3
            if verbose and RANK in {-1, 0}:
                LOGGER.info(f"å•æ¨¡æ€RT-DETRæ¨¡å‹åˆå§‹åŒ–: {self.modality or 'RGB'}(3ch)")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ›´æ–°dataé…ç½®ä»¥ä¿®å¤éªŒè¯å™¨é€šé“æ•°ä¸åŒ¹é…é—®é¢˜
        self.data["channels"] = channels
        if verbose and RANK in {-1, 0}:
            LOGGER.info(f"å·²æ›´æ–°data[\"channels\"]ä¸º{channels}ï¼Œç¡®ä¿éªŒè¯å™¨å…¼å®¹æ€§")
        
        # Create RT-DETR model
        model = RTDETRDetectionModel(
            cfg or self.args.model, 
            nc=self.data["nc"], 
            ch=channels,  # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„é€šé“æ•°
            verbose=verbose and RANK == -1
        )
        
        # åŠ è½½æƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
        if weights:
            model.load(weights)
        
        # TODO: Phase 2 - Integrate MultiModalRouter here
        # if self.is_multimodal and hasattr(model, 'multimodal_router') and model.multimodal_router:
        #     model.multimodal_router.update_dataset_config(self.data)
        
        return model
    
    def preprocess_batch(self, batch):
        """
        Preprocess batch data for multi-modal inputs.
        
        Handles both standard (3-channel) and multi-modal (6-channel) inputs,
        with support for single-modal ablation training.
        
        Args:
            batch (dict): Batch data dictionary containing images and labels.
            
        Returns:
            dict: Preprocessed batch ready for training.
        """
        # Standard preprocessing
        batch = super().preprocess_batch(batch)
        
        if not self.is_multimodal:
            return batch
        
        # Multi-modal specific preprocessing
        if self.modality:  # Single-modal ablation
            # Single-modal ablation: zero out the non-selected modality
            images = batch["img"]  # [B, 6, H, W]

            if self.modality == 'rgb':
                # Zero out X modality (channels 3-5)
                images[:, 3:6, :, :] = 0
                LOGGER.debug("Single-modal RGB training: X modality zeroed")
            else:
                # Zero out RGB modality (channels 0-2)
                images[:, 0:3, :, :] = 0
                LOGGER.debug(f"Single-modal {self.modality} training: RGB modality zeroed")

            batch["img"] = images
        
        return batch
    
    def get_validator(self):
        """
        Return validator compatible with multi-modal RT-DETR.
        
        Returns:
            RTDETRMMValidator: Multi-modal validator instance.
        """
        self.loss_names = "giou_loss", "cls_loss", "l1_loss"  # RT-DETR specific losses
        
        # Import RTDETRMMValidator from multimodal subpackage
        from ultralytics.models.rtdetr.multimodal.val import RTDETRMMValidator
        
        # åˆ›å»ºéªŒè¯å™¨å¹¶ä¼ é€’æ›´æ–°åçš„dataé…ç½®
        validator = RTDETRMMValidator(
            self.test_loader,
            save_dir=self.save_dir,
            args=copy(self.args)
        )
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¼ é€’trainerçš„dataé…ç½®ç»™éªŒè¯å™¨
        # ç¡®ä¿éªŒè¯å™¨ä½¿ç”¨åŒ…å«æ›´æ–°åchannelsä¿¡æ¯çš„dataé…ç½®
        validator.data = self.data
        
        return validator
    
    def _parse_multimodal_config(self):
        """
        Parse and validate multi-modal configuration from data.yaml.
        
        Adapted from YOLOMM's implementation to maintain consistency.
        
        Returns:
            dict: Parsed multi-modal configuration.
        """
        # Single-modal training configuration
        if self.modality:
            if self.modality == 'rgb':
                x_modality = self._determine_x_modality_from_data()
                config = {
                    'models': ['rgb', x_modality],
                    'modalities': {
                        'rgb': 'images',
                        x_modality: f'images_{x_modality}'
                    }
                }
                LOGGER.info(f"RGB single-modal training, X modality: {x_modality}")
            else:
                # å¤„ç† 'X' ç‰¹æ®Šæ ‡è®°ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
                if self.modality.upper() == 'X':
                    # 'X' æ˜¯ç‰¹æ®Šæ ‡è®°ï¼Œéœ€è¦è§£æä¸ºå®é™…çš„Xæ¨¡æ€
                    actual_x_modality = self._determine_x_modality_from_data()
                    # ä»data.yamlè·å–å®é™…çš„è·¯å¾„æ˜ å°„
                    x_modality_path = self._get_x_modality_path(actual_x_modality)
                    
                    config = {
                        'models': ['rgb', actual_x_modality],
                        'modalities': {
                            'rgb': 'images',
                            actual_x_modality: x_modality_path
                        }
                    }
                    LOGGER.info(f"Xæ¨¡æ€å•æ¨¡æ€è®­ç»ƒ: {actual_x_modality}-only (ä»'X'è§£æ)")
                else:
                    # ç”¨æˆ·æŒ‡å®šäº†å…·ä½“çš„æ¨¡æ€åç§°ï¼ˆå¦‚ 'depth', 'thermal', 'ir' ç­‰ï¼‰
                    x_modality_path = self._get_x_modality_path(self.modality)
                    
                    config = {
                        'models': ['rgb', self.modality],
                        'modalities': {
                            'rgb': 'images',
                            self.modality: x_modality_path
                        }
                    }
                    LOGGER.info(f"Xæ¨¡æ€å•æ¨¡æ€è®­ç»ƒ: {self.modality}-only")
            return config
        
        # Dual-modal training configuration
        return self._get_default_multimodal_config()
    
    def _get_default_multimodal_config(self):
        """
        Get default multi-modal configuration from data.yaml.

        Returns:
            dict: Default multi-modal configuration.
        """
        # Check if data is available
        data = getattr(self, 'data', None)

        # Priority 1: modality_used field
        if data and 'modality_used' in data:
            modality_used = data['modality_used']
            if isinstance(modality_used, list) and len(modality_used) >= 2:
                config = {
                    'models': modality_used,
                    'modalities': {}
                }

                # Get path mappings from modality field
                if 'modality' in data and isinstance(data['modality'], dict):
                    modality_paths = data['modality']
                    for mod in modality_used:
                        config['modalities'][mod] = modality_paths.get(
                            mod, 'images' if mod == 'rgb' else f'images_{mod}'
                        )
                else:
                    # Generate default paths
                    for mod in modality_used:
                        config['modalities'][mod] = 'images' if mod == 'rgb' else f'images_{mod}'

                LOGGER.info(f"Loaded multi-modal config: {modality_used}")
                return config

        # Priority 2: models field (backward compatibility)
        if data and 'models' in data:
            models = data['models']
            if isinstance(models, list) and len(models) >= 2:
                config = {'models': models, 'modalities': {}}
                for modality in models:
                    config['modalities'][modality] = 'images' if modality == 'rgb' else f'images_{modality}'
                return config

        # Default fallback
        x_modality = self._determine_x_modality_from_data()
        config = {
            'models': ['rgb', x_modality],
            'modalities': {
                'rgb': 'images',
                x_modality: f'images_{x_modality}'
            }
        }
        LOGGER.info(f"Using default multi-modal config: rgb+{x_modality}")
        return config
    
    def _get_x_modality_path(self, modality_name):
        """
        è·å–æŒ‡å®šæ¨¡æ€çš„å®é™…è·¯å¾„ã€‚
        
        ä¼˜å…ˆä»data.yamlçš„modalityå­—æ®µè¯»å–ï¼Œ
        å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤æ ¼å¼ 'images_{modality_name}'ã€‚
        
        Args:
            modality_name (str): æ¨¡æ€åç§°ï¼ˆå¦‚ 'ir', 'depth', 'thermal'ï¼‰
            
        Returns:
            str: æ¨¡æ€å¯¹åº”çš„ç›®å½•è·¯å¾„
        """
        # ä¼˜å…ˆä»data.yamlçš„modalityå­—æ®µè¯»å–
        if self.data and 'modality' in self.data:
            modality_paths = self.data['modality']
            if isinstance(modality_paths, dict) and modality_name in modality_paths:
                return modality_paths[modality_name]
        
        # å‘åå…¼å®¹ï¼šæ£€æŸ¥modalitieså­—æ®µ
        if self.data and 'modalities' in self.data:
            modalities = self.data['modalities']
            if isinstance(modalities, dict) and modality_name in modalities:
                return modalities[modality_name]
        
        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
        return f'images_{modality_name}'
    
    def _determine_x_modality_from_data(self):
        """
        Intelligently determine X modality type from data configuration.

        Returns:
            str: X modality identifier (e.g., 'depth', 'thermal', 'ir').
        """
        # Safe data access - use getattr to avoid AttributeError during initialization
        data = getattr(self, 'data', None)

        # Check data.yaml for modality information
        if data:
            # Check modality_used
            if 'modality_used' in data:
                modality_used = data['modality_used']
                if isinstance(modality_used, list):
                    x_modalities = [m for m in modality_used if m != 'rgb']
                    if x_modalities:
                        return x_modalities[0]

            # Check models field
            if 'models' in data:
                models = data['models']
                if isinstance(models, list):
                    x_modalities = [m for m in models if m != 'rgb']
                    if x_modalities:
                        return x_modalities[0]
        
        # Default fallback
        LOGGER.warning("Cannot determine X modality type, using default: depth")
        return 'depth'
    
    def _validate_modality_compatibility(self):
        """
        Validate that specified modality is compatible with available data.
        
        Raises:
            ValueError: When modality parameter is incompatible with data configuration.
        """
        if not self.modality:
            return
        
        # Get available modalities
        available_modalities = []
        if self.multimodal_config:
            available_modalities = self.multimodal_config.get('models', [])
        
        # Validate modality compatibility
        if available_modalities:
            # å¤„ç† 'X' ç‰¹æ®Šæ ‡è®°çš„éªŒè¯
            if self.modality.upper() == 'X':
                # 'X' æ˜¯ç‰¹æ®Šæ ‡è®°ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰éRGBçš„Xæ¨¡æ€
                x_modalities = [m for m in available_modalities if m != 'rgb']
                if x_modalities:
                    LOGGER.info(f"âœ… æ¨¡æ€å…¼å®¹æ€§éªŒè¯é€šè¿‡: '{self.modality}' æ˜ å°„åˆ° {x_modalities[0]}")
                else:
                    raise ValueError(
                        f"æŒ‡å®šçš„modality '{self.modality}' æ— æ³•æ˜ å°„åˆ°æœ‰æ•ˆçš„Xæ¨¡æ€ã€‚"
                        f"å¯ç”¨æ¨¡æ€åˆ—è¡¨: {available_modalities}ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°éRGBçš„Xæ¨¡æ€ã€‚"
                    )
            else:
                # æ ‡å‡†æ¨¡æ€éªŒè¯
                if self.modality not in available_modalities:
                    raise ValueError(
                        f"Specified modality '{self.modality}' not in available modalities: {available_modalities}. "
                        f"Please check data configuration or modality parameter."
                    )
                LOGGER.info(f"âœ… Modality compatibility validated: {self.modality} in {available_modalities}")
        else:
            # å¦‚æœæ— æ³•è·å–å¯ç”¨æ¨¡æ€ï¼Œä»…ç»™å‡ºè­¦å‘Š
            LOGGER.warning(f"âš ï¸  æ— æ³•éªŒè¯modality '{self.modality}' çš„å…¼å®¹æ€§ï¼Œæœªæ‰¾åˆ°å¯ç”¨æ¨¡æ€é…ç½®")
    
    def save_model(self):
        """
        Save model with multi-modal configuration information.
        
        Extends parent's save_model to include multi-modal metadata.
        """
        # Call parent's model saving
        super().save_model()
        
        # Save multi-modal configuration to checkpoint
        if self.is_multimodal and hasattr(self, 'multimodal_config'):
            # Update last checkpoint
            ckpt = torch_load(self.last, map_location='cpu')
            ckpt['multimodal_config'] = self.multimodal_config
            ckpt['modality'] = self.modality
            ckpt['is_multimodal'] = True
            torch.save(ckpt, self.last)
            
            # Update best checkpoint if exists
            if self.best.exists():
                ckpt_best = torch_load(self.best, map_location='cpu')
                ckpt_best['multimodal_config'] = self.multimodal_config
                ckpt_best['modality'] = self.modality
                ckpt_best['is_multimodal'] = True
                torch.save(ckpt_best, self.best)
    
    def final_eval(self):
        """
        Perform final evaluation with multi-modal specific logging.
        """
        # Execute standard evaluation
        super().final_eval()
        
        # Log multi-modal specific information
        if self.is_multimodal and hasattr(self, 'multimodal_config') and self.multimodal_config:
            x_modality = [m for m in self.multimodal_config['models'] if m != 'rgb'][0]
            if self.modality:
                LOGGER.info(f"Final evaluation complete - Single-modal training: {self.modality}-only")
            else:
                LOGGER.info(f"Final evaluation complete - Dual-modal training: RGB+{x_modality}")
        else:
            LOGGER.info("Final evaluation complete - Standard RT-DETR training")