# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
RT-DETR MultiModal predictor module.

This module provides the RTDETRMMPredictor class for inference with multi-modal RT-DETR models
supporting RGB+X modality inputs. Architecture strictly follows YOLOMM's successful pattern.
"""

import torch
import numpy as np
import cv2
from pathlib import Path
from typing import Union, List, Dict, Optional, Tuple, Any

from ultralytics.models.rtdetr.predict import RTDETRPredictor
from ultralytics.utils import LOGGER, DEFAULT_CFG, ops, colorstr
from ultralytics.engine.results import Results


class RTDETRMMPredictor(RTDETRPredictor):
    """
    A predictor class for RT-DETR MultiModal (RTDETRMM) object detection models.

    This class extends RTDETRPredictor to support multi-modal inputs (RGB + X modality)
    during inference. Architecture strictly follows YOLOMM's MultiModalDetectionPredictor
    successful pattern for consistency and simplicity.

    Key Features:
        - Supports RGB+X dual-modal inference (best performance)
        - Supports single-modal inference with modality parameter
        - Always outputs 6-channel tensor [X,X,X,RGB,RGB,RGB] format
        - Compatible with YOLOMM API for consistent user experience
        - Simple and clean architecture following proven patterns

    Input Format Requirements:
        - Dual-modal: List format [RGB_source, X_source] - ORDER IS IMPORTANT!
        - Single-modal: Single source with modality parameter
        - 6-channel tensor: Pre-processed [X,X,X,RGB,RGB,RGB] format

    Attributes:
        args: Prediction arguments and settings.
        model: The RTDETRMM model used for inference.
        modality: Specific modality for single-modal inference.
        is_dual_modal: Whether current inference is dual-modal.
        is_single_modal: Whether current inference is single-modal.

    Methods:
        preprocess: Preprocess input data for multi-modal inference.
        _parse_inference_input: Parse and validate inference input.
        _process_dual_modality: Process dual-modal input.
        _process_single_modality: Process single-modal input with filling.

    Examples:
        >>> # Dual-modal inference
        >>> predictor = RTDETRMMPredictor(overrides={'model': 'rtdetr-r18-mm.pt'})
        >>> results = predictor(source=['rgb.jpg', 'thermal.jpg'])  # [RGB, X] order!
        
        >>> # Single-modal inference
        >>> predictor = RTDETRMMPredictor(overrides={'modality': 'rgb'})
        >>> results = predictor(source='rgb.jpg')
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        Initialize RTDETRMMPredictor for multi-modal inference.

        Args:
            cfg (str | Path | dict, optional): Configuration file path or dict.
            overrides (dict, optional): Dictionary to override default prediction arguments.
            _callbacks (list, optional): List of callback functions.
        """
        # Initialize parent predictor
        super().__init__(cfg, overrides, _callbacks)
        
        # Initialize multimodal-specific attributes (following YOLOMM pattern)
        self.modality = getattr(self.args, 'modality', None)
        
        # Initialize modal states (following YOLOMM pattern)
        self.is_dual_modal = self.modality is None
        self.is_single_modal = self.modality is not None
        
        # Track input sources for multi-modal visualization
        self.rgb_source = None
        self.x_source = None
        self.input_mode = None  # 'dual', 'single_rgb', 'single_x'
        
        # Log initialization
        LOGGER.info(f"RTDETRMMPredictor initialized: modality={self.modality}, "
                   f"dual={self.is_dual_modal}, single={self.is_single_modal}")

    def _parse_inference_input(self, source):
        """
        Parse and validate inference input from RTDETRMM.predict() method.
        
        Follows YOLOMM's input parsing pattern exactly for consistency.
        
        Args:
            source: Input source from RTDETRMM.predict() method. Can be:
                - Single file path (str/Path): for single-modal inference
                - List of 2 paths: [rgb_path, x_path] for dual-modal inference
                - PIL.Image or np.ndarray: for single image
                - torch.Tensor: preprocessed tensor
                
        Returns:
            tuple: (parsed_source, input_format_info) where:
                - parsed_source: Validated and normalized input source
                - input_format_info: Dict with input analysis information
                
        Raises:
            ValueError: If input format is invalid or incompatible with modality settings
            TypeError: If input type is not supported
        """
        import numpy as np
        from PIL import Image
        from pathlib import Path
        
        # Initialize input format analysis (following YOLOMM pattern)
        input_info = {
            'input_type': type(source).__name__,
            'is_batch': False,
            'source_count': 1,
            'modality_mode': 'dual' if self.is_dual_modal else f'single_{self.modality}',
            'inference_format': None,
            'validation_passed': False
        }
        
        try:
            # Log initial input analysis
            LOGGER.debug(f"è§£ææ¨ç†è¾“å…¥: ç±»å‹={input_info['input_type']}, æ¨¡æ€æ¨¡å¼={input_info['modality_mode']}")
            
            # Case 1: Tensor input (already preprocessed)
            if isinstance(source, torch.Tensor):
                input_info['inference_format'] = 'preprocessed_tensor'
                input_info['tensor_shape'] = list(source.shape)
                
                if source.dim() == 4 and source.shape[1] == 6:
                    LOGGER.debug("æ£€æµ‹åˆ°6é€šé“é¢„å¤„ç†tensorï¼Œç›´æ¥ä½¿ç”¨")
                    input_info['validation_passed'] = True
                    return source, input_info
                else:
                    LOGGER.warning(f"Tensorç»´åº¦ä¸ç¬¦åˆé¢„æœŸ: {source.shape}ï¼Œå°†é‡æ–°å¤„ç†")
            
            # Case 2: List/Tuple input
            elif isinstance(source, (list, tuple)):
                input_info['source_count'] = len(source)
                
                if len(source) == 2 and self.is_dual_modal:
                    # Dual-modal format: [rgb_source, x_source]
                    input_info['inference_format'] = 'dual_modal'
                    rgb_source, x_source = source
                    
                    # Validate individual sources
                    rgb_info = self._analyze_single_source(rgb_source, 'rgb')
                    x_info = self._analyze_single_source(x_source, 'x_modal')
                    
                    input_info['rgb_source'] = rgb_info
                    input_info['x_source'] = x_info
                    input_info['validation_passed'] = True
                    
                    return source, input_info
                    
                elif len(source) == 1 and self.is_single_modal:
                    # Single-modal with list wrapper
                    input_info['inference_format'] = 'single_modal'
                    single_source = source[0]
                    LOGGER.debug(f"å•æ¨¡æ€è¾“å…¥(åˆ—è¡¨åŒ…è£…): {type(single_source)}")
                    
                    # Analyze the single source
                    source_info = self._analyze_single_source(single_source, self.modality)
                    input_info.update(source_info)
                    input_info['validation_passed'] = True
                    
                    return single_source, input_info
                    
                else:
                    # Invalid list format for current modality settings
                    if self.is_dual_modal:
                        raise ValueError(f"åŒæ¨¡æ€æ¨ç†éœ€è¦2ä¸ªè¾“å…¥æºï¼Œä½†æ¥æ”¶åˆ°{len(source)}ä¸ª")
                    else:
                        # Use first element for single-modal
                        single_source = source[0]
                        LOGGER.warning(f"å•æ¨¡æ€æ¨ç†æ¥æ”¶åˆ°{len(source)}ä¸ªè¾“å…¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª: {single_source}")
                        return self._parse_inference_input(single_source)
            
            # Case 3: Single source input
            else:
                if self.is_dual_modal:
                    raise ValueError(
                        f"åŒæ¨¡æ€æ¨ç†éœ€è¦åˆ—è¡¨æ ¼å¼è¾“å…¥ [rgb_source, x_source]ï¼Œ"
                        f"ä½†æ¥æ”¶åˆ°å•ä¸ªæº: {type(source)}"
                    )
                
                # Single-modal input validation
                input_info['inference_format'] = 'single_modal'
                source_info = self._analyze_single_source(source, self.modality)
                input_info.update(source_info)
                input_info['validation_passed'] = True
                
                return source, input_info
                
        except Exception as e:
            input_info['validation_passed'] = False
            input_info['error'] = str(e)
            LOGGER.error(f"è¾“å…¥è§£æå¤±è´¥: {e}")
            raise
        
        finally:
            # Log final input analysis
            self._log_input_analysis(input_info)

    def _analyze_single_source(self, source, modality_hint=None):
        """
        Analyze a single input source and determine its characteristics.
        Following YOLOMM pattern exactly.
        
        Args:
            source: Single input source (path, PIL.Image, np.ndarray, etc.)
            modality_hint (str): Hint about expected modality type
            
        Returns:
            dict: Analysis information about the source
        """
        import numpy as np
        from PIL import Image
        from pathlib import Path
        
        analysis = {
            'source_type': 'unknown',
            'path': None,
            'exists': False,
            'format': None,
            'modality_hint': modality_hint
        }
        
        if isinstance(source, (str, Path)):
            # File path
            path = Path(source)
            analysis['source_type'] = 'file_path'
            analysis['path'] = str(path)
            analysis['exists'] = path.exists()
            analysis['format'] = path.suffix.lower() if path.suffix else 'no_extension'
            
            if not analysis['exists']:
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                
        elif isinstance(source, Image.Image):
            # PIL Image
            analysis['source_type'] = 'pil_image'
            analysis['format'] = source.format or 'unknown'
            analysis['mode'] = source.mode
            analysis['size'] = source.size
            
        elif isinstance(source, np.ndarray):
            # Numpy array
            analysis['source_type'] = 'numpy_array'
            analysis['shape'] = source.shape
            analysis['dtype'] = str(source.dtype)
            
        elif isinstance(source, torch.Tensor):
            # Tensor
            analysis['source_type'] = 'torch_tensor'
            analysis['shape'] = list(source.shape)
            analysis['dtype'] = str(source.dtype)
            analysis['device'] = str(source.device)
            
        else:
            analysis['source_type'] = f'unsupported_{type(source).__name__}'
            
        return analysis

    def _log_input_analysis(self, input_info):
        """
        Log detailed input analysis information.
        Following YOLOMM pattern.
        
        Args:
            input_info (dict): Input analysis information
        """
        LOGGER.debug("=== RTDETRMMè¾“å…¥è§£æåˆ†ææŠ¥å‘Š ===")
        LOGGER.debug(f"è¾“å…¥ç±»å‹: {input_info['input_type']}")
        LOGGER.debug(f"æ¨ç†æ ¼å¼: {input_info['inference_format']}")
        LOGGER.debug(f"æ¨¡æ€æ¨¡å¼: {input_info['modality_mode']}")
        LOGGER.debug(f"æºæ•°é‡: {input_info['source_count']}")
        LOGGER.debug(f"éªŒè¯é€šè¿‡: {input_info['validation_passed']}")
        
        if 'rgb_source' in input_info:
            LOGGER.debug(f"RGBæºä¿¡æ¯: {input_info['rgb_source']}")
            
        if 'x_source' in input_info:
            LOGGER.debug(f"Xæ¨¡æ€æºä¿¡æ¯: {input_info['x_source']}")
            
        if 'error' in input_info:
            LOGGER.debug(f"é”™è¯¯ä¿¡æ¯: {input_info['error']}")
            
        LOGGER.debug("=== åˆ†ææŠ¥å‘Šç»“æŸ ===")

    def _update_modality_state(self):
        """
        Update modality state dynamically from args (following YOLOMM pattern).
        This allows for dynamic modality switching during prediction.
        """
        current_modality = getattr(self.args, 'modality', None)
        if current_modality != self.modality:
            LOGGER.debug(f"RTDETRMMPredictor: åŠ¨æ€æ›´æ–°æ¨¡æ€çŠ¶æ€ {self.modality} â†’ {current_modality}")
            self.modality = current_modality
            self.is_dual_modal = self.modality is None
            self.is_single_modal = self.modality is not None

    def preprocess(self, im):
        """
        Prepares multimodal input images before inference.
        
        Follows YOLOMM's preprocess pattern exactly for consistency.
        Always outputs 6-channel tensor [X,X,X,RGB,RGB,RGB] format.
        
        Args:
            im (torch.Tensor | List | str): Input images. Can be:
                - List of 2 paths: [rgb_path, x_path] for dual-modal
                - Single path: rgb_path or x_path for single-modal
                - torch.Tensor: preprocessed tensor
                
        Returns:
            torch.Tensor: 6-channel tensor [X, X, X, RGB, RGB, RGB] format
            
        Raises:
            ValueError: If input format is invalid or incompatible with modality settings
            FileNotFoundError: If image files cannot be found
            RuntimeError: If tensor processing fails
        """
        try:
            # Update modality state dynamically (following YOLOMM pattern)
            self._update_modality_state()
            
            # Detect input mode before preprocessing (following YOLOMM pattern)
            if isinstance(im, (list, tuple)) and len(im) == 2:
                self._dual_input_detected = True
                self.input_mode = 'dual'
            else:
                self._dual_input_detected = False
                self.input_mode = f'single_{self.modality}' if self.modality else 'single'
            
            # ä½¿ç”¨æ–°çš„è¾“å…¥è§£ææ–¹æ³• (following YOLOMM pattern)
            LOGGER.debug(f"å¼€å§‹å¤šæ¨¡æ€é¢„å¤„ç†: modality={self.modality}, input_type={type(im)}")
            
            # è§£æå’ŒéªŒè¯è¾“å…¥
            parsed_source, input_info = self._parse_inference_input(im)
            
            # å¿«é€Ÿè·¯å¾„ï¼šå¦‚æœè¾“å…¥å·²ç»æ˜¯æ­£ç¡®æ ¼å¼çš„6é€šé“tensor
            if isinstance(parsed_source, torch.Tensor) and parsed_source.dim() == 4 and parsed_source.shape[1] == 6:
                LOGGER.debug("è¾“å…¥å·²ä¸º6é€šé“tensorï¼Œè¿›è¡Œæ ¼å¼éªŒè¯åç›´æ¥è¿”å›")
                return self._finalize_tensor(parsed_source)
            
            # æ ¹æ®è§£æç»“æœé€‰æ‹©å¤„ç†è·¯å¾„
            if input_info['inference_format'] in ['dual_modal'] and self.is_dual_modal:
                result_tensor = self._process_dual_modality(parsed_source)
            elif input_info['inference_format'] in ['single_modal'] and self.is_single_modal:
                result_tensor = self._process_single_modality(parsed_source)
            else:
                # å…¼å®¹æ—§çš„å¤„ç†æ–¹å¼
                if self.is_dual_modal:
                    result_tensor = self._process_dual_modality(parsed_source)
                else:
                    result_tensor = self._process_single_modality(parsed_source)
            
            # æœ€ç»ˆæ ¼å¼éªŒè¯å’Œè®¾å¤‡è½¬æ¢
            final_tensor = self._finalize_tensor(result_tensor)
            
            LOGGER.debug(f"å¤šæ¨¡æ€é¢„å¤„ç†å®Œæˆ: shape={final_tensor.shape}, device={final_tensor.device}")
            return final_tensor
            
        except Exception as e:
            # ç»Ÿä¸€å¼‚å¸¸å¤„ç†
            error_msg = f"å¤šæ¨¡æ€é¢„å¤„ç†å¤±è´¥: {str(e)}"
            LOGGER.error(error_msg)
            self._log_debug_info(im, e)
            raise RuntimeError(error_msg) from e

    def _process_dual_modality(self, im):
        """
        Process dual-modal input: [rgb_path, x_path] or similar formats.
        
        Follows YOLOMM pattern exactly with RT-DETR specific adaptations.
        
        Args:
            im (List): Dual-modal input data [RGB_source, X_source]
            
        Returns:
            torch.Tensor: 6-channel tensor [X, X, X, RGB, RGB, RGB]
        """
        # If already preprocessed tensor with 6 channels, return directly
        if isinstance(im, torch.Tensor) and im.shape[1] == 6:
            LOGGER.debug("è¾“å…¥å·²ä¸º6é€šé“tensorï¼Œç›´æ¥è¿”å›")
            return im
        
        # Parse dual-modal input and load images
        rgb_images, x_images = self._parse_dual_modal_input(im)
        
        # Preprocess each modality separately using parent's method (RT-DETR specific)
        rgb_tensor = super().preprocess(rgb_images)  # Shape: (B, 3, H, W)
        x_tensor = super().preprocess(x_images)      # Shape: (B, 3, H, W)
        
        # Ensure same spatial dimensions
        rgb_tensor, x_tensor = self._align_tensor_dimensions(rgb_tensor, x_tensor)
        
        # Combine modalities: [X, X, X, RGB, RGB, RGB] order (matching YOLOMM training)
        combined_tensor = torch.cat([x_tensor, rgb_tensor], dim=1)  # Shape: (B, 6, H, W)
        
        LOGGER.debug(f"åŒæ¨¡æ€é¢„å¤„ç†å®Œæˆ: {combined_tensor.shape}")
        return combined_tensor

    def _parse_dual_modal_input(self, im):
        """
        Parse dual-modal input and separate RGB and X-modal data.
        Following YOLOMM pattern with RT-DETR adaptations.
        
        Args:
            im (List): Input data - [rgb_source, x_source]
                
        Returns:
            tuple: (rgb_images, x_images) ready for preprocessing
        """
        if isinstance(im, (list, tuple)):
            if len(im) == 2:
                # Standard dual-modal format: [rgb_source, x_source]
                rgb_source, x_source = im
                LOGGER.debug(f"è§£ææ ‡å‡†åŒæ¨¡æ€è¾“å…¥: RGB={type(rgb_source)}, X={type(x_source)}")
                
                # Load images using simple loading
                rgb_images = self._load_image_source(rgb_source)
                x_images = self._load_image_source(x_source)
                
                return rgb_images, x_images
                
            else:
                # Invalid dual-modal input format
                raise ValueError(
                    f"åŒæ¨¡æ€æ¨ç†éœ€è¦åŒ…å«2ä¸ªå…ƒç´ çš„åˆ—è¡¨è¾“å…¥ [rgb_source, x_source]ï¼Œ"
                    f"ä½†æ¥æ”¶åˆ°: {type(im)} with {len(im)} å…ƒç´ "
                )
        else:
            # Single source input - invalid for dual-modal
            raise ValueError(
                f"åŒæ¨¡æ€æ¨ç†éœ€è¦åˆ—è¡¨æ ¼å¼è¾“å…¥ [rgb_source, x_source]ï¼Œ"
                f"ä½†æ¥æ”¶åˆ°å•ä¸ªæº: {type(im)}"
            )

    def _load_image_source(self, source):
        """
        Load image(s) from various source types.
        Simplified version of YOLOMM's loading mechanism.
        
        Args:
            source (str | Path | PIL.Image | np.ndarray | torch.Tensor): Image source
            
        Returns:
            List[np.ndarray]: Loaded images ready for preprocessing
        """
        import cv2
        import numpy as np
        from PIL import Image
        from pathlib import Path
        
        LOGGER.debug(f"åŠ è½½å›¾åƒæº: ç±»å‹={type(source)}")
        
        # Case 1: String or Path (file path)
        if isinstance(source, (str, Path)):
            source_path = Path(source)
            
            # Check if file exists
            if not source_path.exists():
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {source_path}")
            
            # Direct OpenCV loading
            img = cv2.imread(str(source_path))
            if img is None:
                raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {source_path}")
            
            # Convert BGR to RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            LOGGER.debug(f"æˆåŠŸåŠ è½½å›¾åƒ: {source_path}")
            return [img]
            
        # Case 2: PIL Image
        elif isinstance(source, Image.Image):
            LOGGER.debug("å¤„ç†PILå›¾åƒè¾“å…¥")
            if source.mode != "RGB":
                source = source.convert("RGB")
            img = np.asarray(source)
            return [img]
            
        # Case 3: Numpy array
        elif isinstance(source, np.ndarray):
            LOGGER.debug(f"å¤„ç†numpyæ•°ç»„è¾“å…¥: shape={source.shape}")
            
            if source.ndim == 3:
                # Single image: (H, W, C)
                return [source]
            elif source.ndim == 4:
                # Batch of images: (B, H, W, C) or (B, C, H, W)
                if source.shape[1] == 3 or source.shape[1] == 1:
                    # Format: (B, C, H, W) -> convert to (B, H, W, C)
                    images = [img.transpose(1, 2, 0) for img in source]
                else:
                    # Format: (B, H, W, C)
                    images = list(source)
                return images
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„numpyæ•°ç»„ç»´åº¦: {source.shape}")
                
        # Case 4: Torch Tensor
        elif isinstance(source, torch.Tensor):
            LOGGER.debug(f"å¤„ç†torch.Tensorè¾“å…¥: shape={source.shape}")
            
            # Convert to numpy
            if source.device != torch.device('cpu'):
                source = source.cpu()
            source_np = source.numpy()
            
            # Recursive call with numpy array
            return self._load_image_source(source_np)
            
        # Case 5: List or Tuple (multiple images)
        elif isinstance(source, (list, tuple)):
            LOGGER.debug(f"å¤„ç†åˆ—è¡¨è¾“å…¥: é•¿åº¦={len(source)}")
            
            all_images = []
            for i, item in enumerate(source):
                try:
                    loaded = self._load_image_source(item)
                    all_images.extend(loaded)
                except Exception as e:
                    LOGGER.error(f"åŠ è½½åˆ—è¡¨é¡¹[{i}]å¤±è´¥: {e}")
                    raise
            
            LOGGER.debug(f"åˆ—è¡¨åŠ è½½å®Œæˆ: æ€»è®¡{len(all_images)}å¼ å›¾åƒ")
            return all_images
            
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„å›¾åƒæºç±»å‹: {type(source)}")

    def _align_tensor_dimensions(self, tensor1, tensor2):
        """
        Ensure two tensors have the same spatial dimensions.
        Following YOLOMM pattern exactly.
        
        Args:
            tensor1 (torch.Tensor): First tensor (B, C, H, W)
            tensor2 (torch.Tensor): Second tensor (B, C, H, W)
            
        Returns:
            tuple: (aligned_tensor1, aligned_tensor2) with same spatial dimensions
        """
        import torch.nn.functional as F
        
        if tensor1.shape[2:] == tensor2.shape[2:]:
            # Already same dimensions
            return tensor1, tensor2
        
        # Get dimensions
        h1, w1 = tensor1.shape[2:]
        h2, w2 = tensor2.shape[2:]
        
        # Use the smaller dimensions as target
        target_h = min(h1, h2)
        target_w = min(w1, w2)
        target_size = (target_h, target_w)
        
        LOGGER.debug(f"å¯¹é½tensorç»´åº¦åˆ°: {target_size}")
        
        # Resize if necessary
        if (h1, w1) != target_size:
            tensor1 = F.interpolate(tensor1, size=target_size, mode='bilinear', align_corners=False)
        
        if (h2, w2) != target_size:
            tensor2 = F.interpolate(tensor2, size=target_size, mode='bilinear', align_corners=False)
        
        return tensor1, tensor2

    def _process_single_modality(self, im):
        """
        Process single-modal input with intelligent filling for missing modality.
        
        Follows YOLOMM pattern exactly with ModalityFiller integration.
        
        Args:
            im (str | Path | PIL.Image | np.ndarray | torch.Tensor): Single modal input
            
        Returns:
            torch.Tensor: 6-channel tensor [X, X, X, RGB, RGB, RGB] format
        """
        # å¦‚æœè¾“å…¥æ˜¯è·¯å¾„å­—ç¬¦ä¸²ï¼Œéœ€è¦å…ˆåŠ è½½å›¾åƒ
        if isinstance(im, (str, Path)):
            im = self._load_image_source(im)
        
        # ç¡®ä¿ä¼ é€’ç»™çˆ¶ç±»preprocessçš„æ˜¯æ­£ç¡®æ ¼å¼çš„åˆ—è¡¨
        if isinstance(im, np.ndarray) and im.ndim == 3:
            # å•ä¸ªHWCå›¾åƒéœ€è¦åŒ…è£…æˆåˆ—è¡¨æ ¼å¼
            im = [im]
        
        # Preprocess the input modality using parent's method (RT-DETR specific)
        source_tensor = super().preprocess(im)  # Shape: (B, 3, H, W)
        
        if self.modality == 'rgb':
            # RGBæ¨¡æ€æ¨ç†ï¼šRGBä¸ºçœŸå®æ•°æ®ï¼ŒXæ¨¡æ€éœ€è¦å¡«å……
            rgb_tensor = source_tensor
            x_tensor = self._generate_modality_filling(rgb_tensor, 'rgb', 'x')
            LOGGER.debug(f"RGBå•æ¨¡æ€æ¨ç† - ç”ŸæˆXæ¨¡æ€å¡«å……æ•°æ®")
            
        elif self.modality in ['depth', 'thermal', 'ir', 'nir', 'x']:
            # Xæ¨¡æ€æ¨ç†ï¼šXæ¨¡æ€ä¸ºçœŸå®æ•°æ®ï¼ŒRGBéœ€è¦å¡«å……
            x_tensor = source_tensor
            rgb_tensor = self._generate_modality_filling(x_tensor, self.modality, 'rgb')
            LOGGER.debug(f"{self.modality}å•æ¨¡æ€æ¨ç† - ç”ŸæˆRGBå¡«å……æ•°æ®")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å•æ¨¡æ€ç±»å‹: {self.modality}")
        
        # ç»„åˆæˆ6é€šé“tensor: [X, X, X, RGB, RGB, RGB] é¡ºåºï¼ˆä¸YOLOMMè®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        combined_tensor = torch.cat([x_tensor, rgb_tensor], dim=1)  # Shape: (B, 6, H, W)
        
        LOGGER.debug(f"å•æ¨¡æ€é¢„å¤„ç†å®Œæˆ: {combined_tensor.shape}")
        return combined_tensor

    def _generate_modality_filling(self, source_tensor: torch.Tensor, 
                                   source_modality: str, target_modality: str) -> torch.Tensor:
        """
        Generate filled modality data for single-modal inference.
        
        Uses YOLOMM's ModalityFiller if available, otherwise simple fallback.
        
        Args:
            source_tensor: Source modality tensor (B, 3, H, W)
            source_modality: Source modality name
            target_modality: Target modality to generate
            
        Returns:
            torch.Tensor: Generated modality tensor (B, 3, H, W)
        """
        # Try to import YOLOMM's ModalityFiller if available
        try:
            from ultralytics.models.yolo.multimodal.modal_filling import generate_modality_filling
            LOGGER.debug(f"ä½¿ç”¨YOLOMMçš„ModalityFillerç”Ÿæˆ{target_modality}æ¨¡æ€")
            return generate_modality_filling(source_tensor, source_modality, target_modality)
        except ImportError:
            LOGGER.debug(f"ModalityFillerä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•å¡«å……ç­–ç•¥")
        
        # Fallback: Simple filling strategies
        if target_modality == 'x' or source_modality == 'rgb':
            # For X modality, use edge detection or grayscale as simple approximation
            # Convert to grayscale and replicate
            gray = 0.299 * source_tensor[:, 0] + 0.587 * source_tensor[:, 1] + 0.114 * source_tensor[:, 2]
            filled = gray.unsqueeze(1).repeat(1, 3, 1, 1)
        else:
            # For RGB from X modality, just copy (simple strategy)
            filled = source_tensor.clone()
        
        # Add slight noise to indicate it's synthetic
        noise = torch.randn_like(filled) * 0.01
        filled = filled + noise
        
        return filled.clamp(0, 1)

    def _finalize_tensor(self, tensor):
        """
        Finalize processed tensor with format validation and device management.
        Following YOLOMM pattern exactly.
        
        Args:
            tensor (torch.Tensor): Processed tensor to finalize
            
        Returns:
            torch.Tensor: Finalized tensor on correct device
            
        Raises:
            ValueError: If tensor format is invalid
        """
        if not isinstance(tensor, torch.Tensor):
            raise ValueError(f"æœŸæœ›torch.Tensorè¾“å‡ºï¼Œä½†å¾—åˆ°: {type(tensor)}")
        
        if tensor.dim() != 4:
            raise ValueError(f"æœŸæœ›4ç»´tensor [B, C, H, W]ï¼Œä½†å¾—åˆ°ç»´åº¦: {tensor.dim()}")
        
        if tensor.shape[1] != 6:
            raise ValueError(f"æœŸæœ›6é€šé“tensorï¼Œä½†å¾—åˆ°: {tensor.shape[1]}é€šé“")
        
        # è®¾å¤‡ç®¡ç† - ç¡®ä¿tensoråœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self, 'device') and self.device != tensor.device:
            LOGGER.debug(f"è½¬ç§»tensoråˆ°è®¾å¤‡: {self.device}")
            tensor = tensor.to(self.device)
        
        # æ•°æ®ç±»å‹ç¡®ä¿
        if tensor.dtype != torch.float32:
            LOGGER.debug(f"è½¬æ¢tensoræ•°æ®ç±»å‹: {tensor.dtype} -> torch.float32")
            tensor = tensor.float()
        
        return tensor

    def _log_debug_info(self, im, exception):
        """
        Log detailed debug information when preprocessing fails.
        Following YOLOMM pattern.
        
        Args:
            im: Original input data
            exception: The exception that occurred
        """
        LOGGER.debug("=== å¤šæ¨¡æ€é¢„å¤„ç†è°ƒè¯•ä¿¡æ¯ ===")
        LOGGER.debug(f"æ¨¡æ€è®¾ç½®: modality={self.modality}, is_dual_modal={self.is_dual_modal}")
        LOGGER.debug(f"è¾“å…¥ç±»å‹: {type(im)}")
        
        if isinstance(im, (list, tuple)):
            LOGGER.debug(f"åˆ—è¡¨è¾“å…¥é•¿åº¦: {len(im)}")
            for i, item in enumerate(im):
                LOGGER.debug(f"  é¡¹ç›®[{i}]: {type(item)} - {item}")
        elif isinstance(im, torch.Tensor):
            LOGGER.debug(f"Tensorå½¢çŠ¶: {im.shape}")
            LOGGER.debug(f"Tensorè®¾å¤‡: {im.device}")
            LOGGER.debug(f"Tensoræ•°æ®ç±»å‹: {im.dtype}")
        else:
            LOGGER.debug(f"è¾“å…¥å†…å®¹: {im}")
        
        LOGGER.debug(f"å¼‚å¸¸ç±»å‹: {type(exception).__name__}")
        LOGGER.debug(f"å¼‚å¸¸ä¿¡æ¯: {str(exception)}")
        LOGGER.debug("=== è°ƒè¯•ä¿¡æ¯ç»“æŸ ===")

    def stream_inference(self, source=None, model=None, *args, **kwargs):
        """
        Streams real-time inference on camera feed and saves results to file.
        
        Overrides parent method to handle 6-channel warmup for multimodal RT-DETR models.
        Always uses 6-channel warmup for RTDETRMM (matching YOLOMM pattern).
        
        Args:
            source (str, optional): The source of the image to make predictions on.
            model (nn.Module, optional): The model to use for predictions.
            *args (Any): Variable length argument list.
            **kwargs (Any): Arbitrary keyword arguments.
            
        Yields:
            (List[ultralytics.engine.results.Results]): The prediction results.
        """
        if self.args.verbose:
            LOGGER.info("")

        # Setup model
        if not self.model:
            self.setup_model(model)

        with self._lock:  # for thread-safe inference
            # Setup source every time predict is called
            self.setup_source(source if source is not None else self.args.source)

            # Check if save_dir/ label file exists
            if self.args.save or self.args.save_txt:
                (self.save_dir / "labels" if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            # RTDETRMM warmup with 6 channels (following YOLOMM pattern)
            if not self.done_warmup:
                model_channels = 6  # RTDETRMMå›ºå®šä¸º6é€šé“è¾“å…¥ï¼ˆä¸YOLOMMä¸€è‡´ï¼‰
                LOGGER.info(f"RTDETRMMPredictor: ä½¿ç”¨ {model_channels} é€šé“è¿›è¡Œæ¨¡å‹é¢„çƒ­")
                
                # RT-DETR multimodal warmup with 6 channels
                self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 
                                       model_channels, *self.imgsz))
                self.done_warmup = True

            self.seen, self.windows, self.batch = 0, [], None
            profilers = (
                ops.Profile(device=self.device),
                ops.Profile(device=self.device),
                ops.Profile(device=self.device),
            )
            self.run_callbacks("on_predict_start")
            for self.batch in self.dataset:
                self.run_callbacks("on_predict_batch_start")
                paths, im0s, s = self.batch

                # Preprocess
                with profilers[0]:
                    im = self.preprocess(im0s)

                # Inference
                with profilers[1]:
                    preds = self.inference(im, *args, **kwargs)
                    if self.args.embed:
                        yield from [preds] if isinstance(preds, torch.Tensor) else preds  # yield embedding tensors
                        continue

                # Postprocess
                with profilers[2]:
                    self.results = self.postprocess(preds, im, im0s)
                self.run_callbacks("on_predict_postprocess_end")

                # Visualize, save, write results
                n = len(im0s)  # åŸå§‹è¾“å…¥å›¾åƒæ•°é‡
                results_count = len(self.results)  # å®é™…ç”Ÿæˆçš„ç»“æœæ•°é‡
                
                # å¯¹äºå¤šæ¨¡æ€æ¨ç†ï¼ŒåŸå§‹å›¾åƒå¯èƒ½æ˜¯2å¼ ï¼Œä½†ç»“æœåªæœ‰1ä¸ª
                # éœ€è¦æ ¹æ®å®é™…resultsæ•°é‡æ¥å¤„ç†
                if results_count != n:
                    LOGGER.debug(f"å¤šæ¨¡æ€æ¨ç†: è¾“å…¥{n}å¼ å›¾åƒï¼Œç”Ÿæˆ{results_count}ä¸ªç»“æœ")
                
                for i in range(results_count):  # ä½¿ç”¨å®é™…ç»“æœæ•°é‡
                    self.seen += 1
                    self.results[i].speed = {
                        "preprocess": profilers[0].dt * 1e3 / results_count,  # ä½¿ç”¨ç»“æœæ•°é‡è®¡ç®—å¹³å‡æ—¶é—´
                        "inference": profilers[1].dt * 1e3 / results_count,
                        "postprocess": profilers[2].dt * 1e3 / results_count,
                    }
                    
                    # ä¸ºå¤šæ¨¡æ€æ¨ç†è°ƒæ•´è·¯å¾„å¤„ç†
                    if results_count < n:
                        # å¤šæ¨¡æ€æƒ…å†µï¼šå¤šä¸ªè¾“å…¥äº§ç”Ÿ1ä¸ªç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè·¯å¾„ä½œä¸ºä¸»è·¯å¾„
                        result_path = Path(paths[0])
                        result_string = s[0] if s else ""
                        
                        # åœ¨ç»“æœå­—ç¬¦ä¸²ä¸­æ·»åŠ å¤šæ¨¡æ€ä¿¡æ¯
                        if len(paths) > 1:
                            modality_info = f"({len(paths)}æ¨¡æ€è¾“å…¥)"
                            result_string = f"{result_string} {modality_info}" if result_string else modality_info
                    else:
                        # æ ‡å‡†æƒ…å†µï¼š1å¯¹1æ˜ å°„
                        result_path = Path(paths[i])
                        result_string = s[i] if i < len(s) else ""
                    
                    if self.args.verbose or self.args.save or self.args.save_txt or self.args.show:
                        result_string += self.write_results(i, result_path, im, result_string)
                    
                    # ä¿å­˜æ›´æ–°åçš„å­—ç¬¦ä¸²
                    if i < len(s):
                        s[i] = result_string
                    elif len(s) == 0:
                        s = [result_string]

                # Print batch results
                if self.args.verbose:
                    # åªæ‰“å°æœ‰æ•ˆçš„ç»“æœå­—ç¬¦ä¸²
                    valid_strings = [s_item for s_item in s[:results_count] if s_item]
                    if valid_strings:
                        LOGGER.info("\n".join(valid_strings))

                self.run_callbacks("on_predict_batch_end")
                yield from self.results

        # Release assets
        for v in self.vid_writer.values():
            if isinstance(v, cv2.VideoWriter):
                v.release()

        # Print final results
        if self.args.verbose and self.seen:
            t = tuple(x.t / self.seen * 1e3 for x in profilers)  # speeds per image
            LOGGER.info(
                f"Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess per image at shape "
                f"{(min(self.args.batch, self.seen), 6, *im.shape[2:])}"  # æ˜¾ç¤º6é€šé“
                % t
            )
        if self.args.save or self.args.save_txt or self.args.save_crop:
            nl = len(list(self.save_dir.glob("labels/*.txt")))  # number of labels
            s = f"\n{nl} label{'s' * (nl > 1)} saved to {self.save_dir / 'labels'}" if self.args.save_txt else ""
            LOGGER.info(f"Results saved to {colorstr('bold', self.save_dir)}{s}")
        self.run_callbacks("on_predict_end")
    
    def postprocess(self, preds, img, orig_imgs):
        """
        Override parent's postprocess to cache original images for multi-modal visualization.
        
        Following YOLOMM pattern for multi-modal result handling.
        For dual-modal input, creates only one Results object using the RGB path.
        
        Args:
            preds: Model predictions
            img: Preprocessed images
            orig_imgs: Original images
            
        Returns:
            Results: Detection results
        """
        # Store original images for later use in visualization
        self._orig_imgs_cache = orig_imgs
        
        # Call parent's postprocess to get properly formatted results
        results = super().postprocess(preds, img, orig_imgs)
        
        # For dual-modal input, only keep the first result (RGB)
        # This prevents duplicate outputs since we have 2 inputs but 1 detection result
        if self.is_dual_modal and isinstance(orig_imgs, list) and len(orig_imgs) == 2 and len(results) == 2:
            # Keep only the first result (RGB), as both share the same detections
            results = results[:1]
            LOGGER.debug("RTDETRMMPredictor: åŒæ¨¡æ€è¾“å…¥åˆå¹¶ä¸ºå•ä¸ªç»“æœå¯¹è±¡")
        
        return results
    
    def write_results(self, i: int, p: Path, im: torch.Tensor, s: list) -> str:
        """
        Override parent's write_results to handle multi-modal result saving.
        
        Following YOLOMM pattern exactly:
        For dual-modal input: saves RGB result, X result, and side-by-side comparison
        For single-modal input: saves only the specified modality result
        
        Args:
            i (int): Index of the current result in the batch
            p (Path): Path to the original image file
            im (torch.Tensor): Preprocessed 6-channel tensor [X,X,X,RGB,RGB,RGB]
            s (list): List of result strings
            
        Returns:
            str: Result string with processing information
        """
        string = super().write_results(i, p, im, s)
        
        # Only process if saving is enabled
        if self.args.save:
            # Determine save mode based on input
            if self.is_dual_modal and hasattr(self, '_dual_input_detected') and self._dual_input_detected:
                # Dual-modal input: save RGB, X, and combined results
                self._save_multimodal_results(i, p, im)
            elif self.is_single_modal:
                # Single-modal input: modify filename to include modality
                self._update_single_modal_filename(p)
        
        return string
    
    def _save_multimodal_results(self, i: int, p: Path, im: torch.Tensor):
        """
        Save separate RGB and X modality results for dual-modal input.
        
        Creates three output files:
        1. filename.jpg - RGB modality inference result (following YOLOMM naming)
        2. filename_X.jpg - X modality inference result  
        3. filename_multimodal.jpg - Side-by-side comparison
        
        Args:
            i (int): Result index
            p (Path): Original image path
            im (torch.Tensor): 6-channel preprocessed tensor
        """
        try:
            # Get the result object
            result = self.results[i]
            
            # Get base filename without extension
            base_name = p.stem
            save_dir = self.save_dir
            
            # Get original images from cache if available
            if hasattr(self, '_orig_imgs_cache') and self._orig_imgs_cache is not None:
                # For dual-modal input, we should have RGB and X images
                if len(self._orig_imgs_cache) >= 2:
                    # First image is RGB, second is X modality
                    rgb_img = self._orig_imgs_cache[0]
                    x_img = self._orig_imgs_cache[1]
                    
                    # Ensure images are in RGB format
                    if isinstance(rgb_img, np.ndarray):
                        if rgb_img.ndim == 2:
                            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_GRAY2RGB)
                        elif rgb_img.shape[2] == 4:
                            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGRA2RGB)
                        elif rgb_img.shape[2] == 3 and rgb_img.dtype == np.uint8:
                            # Assume BGR format from cv2.imread
                            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
                    
                    if isinstance(x_img, np.ndarray):
                        if x_img.ndim == 2:
                            x_img = cv2.cvtColor(x_img, cv2.COLOR_GRAY2RGB)
                        elif x_img.shape[2] == 4:
                            x_img = cv2.cvtColor(x_img, cv2.COLOR_BGRA2RGB)
                        elif x_img.shape[2] == 3 and x_img.dtype == np.uint8:
                            # Assume BGR format from cv2.imread
                            x_img = cv2.cvtColor(x_img, cv2.COLOR_BGR2RGB)
                else:
                    # Fallback to tensor conversion if not enough original images
                    LOGGER.warning("ä½¿ç”¨tensoræ¢å¤å›¾åƒä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
                    rgb_tensor, x_tensor = self._separate_modalities(im)
                    rgb_img = self._tensor_to_image(rgb_tensor)
                    x_img = self._tensor_to_image(x_tensor)
            else:
                # Fallback to tensor conversion
                LOGGER.warning("åŸå§‹å›¾åƒç¼“å­˜ä¸å¯ç”¨ï¼Œä½¿ç”¨tensoræ¢å¤")
                rgb_tensor, x_tensor = self._separate_modalities(im)
                rgb_img = self._tensor_to_image(rgb_tensor)
                x_img = self._tensor_to_image(x_tensor)
            
            # Plot results on each modality
            # RGB modality result (ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼Œä¸åŠ åç¼€)
            rgb_annotated = self._plot_on_image(result, rgb_img, "RGB")
            rgb_path = save_dir / f"{base_name}.jpg"  # æŒ‰ç…§è®¾è®¡ï¼ŒRGBé¢„æµ‹å›¾ä½¿ç”¨åŸå§‹æ–‡ä»¶å
            cv2.imwrite(str(rgb_path), cv2.cvtColor(rgb_annotated, cv2.COLOR_RGB2BGR))
            
            # X modality result
            x_annotated = self._plot_on_image(result, x_img, "X")
            x_path = save_dir / f"{base_name}_X.jpg"
            cv2.imwrite(str(x_path), cv2.cvtColor(x_annotated, cv2.COLOR_RGB2BGR))
            
            # Create side-by-side comparison  
            multimodal_img = self._create_multimodal_comparison(rgb_annotated, x_annotated)
            multimodal_path = save_dir / f"{base_name}_multimodal.jpg"  # å°å†™multimodal
            cv2.imwrite(str(multimodal_path), cv2.cvtColor(multimodal_img, cv2.COLOR_RGB2BGR))
            
            LOGGER.info(f"ä¿å­˜å¤šæ¨¡æ€ç»“æœ: RGB={rgb_path}, X={x_path}, å¯¹æ¯”å›¾={multimodal_path}")
            
        except Exception as e:
            LOGGER.error(f"ä¿å­˜å¤šæ¨¡æ€ç»“æœå¤±è´¥: {e}")
    
    def _update_single_modal_filename(self, p: Path):
        """
        Update the saved filename for single-modal input to include modality.
        
        Args:
            p (Path): Original image path
        """
        try:
            # Wait a bit to ensure file is saved
            import time
            time.sleep(0.2)
            
            # Get the default save path
            default_path = self.save_dir / p.name
            default_jpg_path = default_path.with_suffix('.jpg')
            
            # Create new filename with modality
            base_name = p.stem
            modality_upper = self.modality.upper() if self.modality else "UNKNOWN"
            new_path = self.save_dir / f"{base_name}_{modality_upper}.jpg"
            
            # Also check for the file with original extension
            original_ext_path = self.save_dir / p.name
            
            # Try multiple possible default filenames
            possible_files = [default_jpg_path, original_ext_path]
            
            for default_file in possible_files:
                if default_file.exists() and not new_path.exists():
                    # Rename the file
                    default_file.rename(new_path)
                    LOGGER.info(f"å•æ¨¡æ€æ–‡ä»¶é‡å‘½åæˆåŠŸ: {default_file.name} -> {new_path.name}")
                    break
            else:
                # If no file found to rename, log debug info
                LOGGER.warning(f"å•æ¨¡æ€æ–‡ä»¶é‡å‘½åå¤±è´¥: æœªæ‰¾åˆ°é»˜è®¤ä¿å­˜çš„æ–‡ä»¶")
                LOGGER.debug(f"å°è¯•æŸ¥æ‰¾çš„æ–‡ä»¶: {[str(f) for f in possible_files]}")
                LOGGER.debug(f"ç›®å½•ä¸­çš„æ–‡ä»¶: {list(self.save_dir.glob('*'))}")
                
        except Exception as e:
            LOGGER.error(f"æ›´æ–°å•æ¨¡æ€æ–‡ä»¶åå¤±è´¥: {e}")
    
    def _separate_modalities(self, tensor: torch.Tensor) -> tuple:
        """
        Separate 6-channel tensor into RGB and X modality tensors.
        
        Args:
            tensor (torch.Tensor): 6-channel tensor [X,X,X,RGB,RGB,RGB]
            
        Returns:
            tuple: (rgb_tensor, x_tensor) each with 3 channels
        """
        if tensor.dim() == 3:
            # Single image: (6, H, W)
            x_tensor = tensor[:3]    # First 3 channels (X modality)
            rgb_tensor = tensor[3:]  # Last 3 channels (RGB)
        else:
            # Batch: (B, 6, H, W)
            x_tensor = tensor[:, :3]    # X modality channels
            rgb_tensor = tensor[:, 3:]  # RGB channels
            
        return rgb_tensor, x_tensor
    
    def _tensor_to_image(self, tensor: torch.Tensor) -> np.ndarray:
        """
        Convert preprocessed tensor back to displayable image.
        
        Handles denormalization and format conversion.
        
        Args:
            tensor (torch.Tensor): Normalized tensor (C, H, W) or (B, C, H, W)
            
        Returns:
            np.ndarray: Image in HWC format with uint8 values
        """
        # Remove batch dimension if present
        if tensor.dim() == 4 and tensor.shape[0] == 1:
            tensor = tensor[0]
        elif tensor.dim() == 4:
            # For batch, take first image
            tensor = tensor[0]
            
        # Move to CPU if on GPU
        tensor = tensor.cpu()
        
        # Convert to HWC format
        img = tensor.permute(1, 2, 0).numpy()
        
        # Denormalize (assume standard ImageNet normalization)
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img = img * std + mean
        
        # Clip to [0, 1] and convert to uint8
        img = np.clip(img, 0, 1)
        img = (img * 255).astype(np.uint8)
        
        # Handle grayscale
        if img.shape[2] == 1:
            img = cv2.cvtColor(img.squeeze(), cv2.COLOR_GRAY2RGB)
            
        return img
    
    def _plot_on_image(self, result, img: np.ndarray, modality_name: str) -> np.ndarray:
        """
        Plot detection results on a specific modality image.
        
        Args:
            result: Detection result object
            img (np.ndarray): Image to plot on (HWC format)
            modality_name (str): Name of the modality for labeling
            
        Returns:
            np.ndarray: Annotated image
        """
        # Create a copy to avoid modifying original
        img_copy = img.copy()
        
        # Use the result's plot method with the specific image
        annotated = result.plot(
            img=img_copy,
            line_width=self.args.line_width,
            boxes=self.args.show_boxes,
            conf=self.args.show_conf,
            labels=self.args.show_labels
        )
        
        # Add modality label
        h, w = annotated.shape[:2]
        label_bg_color = (0, 0, 0)  # Black background
        label_text_color = (255, 255, 255)  # White text
        
        # Draw modality label in top-left corner
        cv2.rectangle(annotated, (10, 10), (150, 40), label_bg_color, -1)
        cv2.putText(annotated, f"{modality_name} Modality", (15, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, label_text_color, 2)
        
        return annotated
    
    def _create_multimodal_comparison(self, rgb_img: np.ndarray, x_img: np.ndarray) -> np.ndarray:
        """
        Create side-by-side comparison of RGB and X modality results.
        
        Args:
            rgb_img (np.ndarray): Annotated RGB image
            x_img (np.ndarray): Annotated X modality image
            
        Returns:
            np.ndarray: Combined side-by-side image
        """
        # Ensure both images have the same height
        h1, w1 = rgb_img.shape[:2]
        h2, w2 = x_img.shape[:2]
        
        if h1 != h2:
            # Resize to match heights
            target_h = max(h1, h2)
            if h1 < target_h:
                scale = target_h / h1
                new_w1 = int(w1 * scale)
                rgb_img = cv2.resize(rgb_img, (new_w1, target_h))
            else:
                scale = target_h / h2
                new_w2 = int(w2 * scale)
                x_img = cv2.resize(x_img, (new_w2, target_h))
        
        # Create side-by-side image
        gap = 10  # Gap between images
        combined_width = rgb_img.shape[1] + x_img.shape[1] + gap
        combined_height = max(rgb_img.shape[0], x_img.shape[0])
        
        # Create black canvas
        combined = np.zeros((combined_height, combined_width, 3), dtype=np.uint8)
        
        # Place images
        combined[:rgb_img.shape[0], :rgb_img.shape[1]] = rgb_img
        combined[:x_img.shape[0], rgb_img.shape[1] + gap:] = x_img
        
        # Add title
        title_height = 50
        final_img = np.zeros((combined_height + title_height, combined_width, 3), dtype=np.uint8)
        final_img[title_height:] = combined
        
        # Add title text
        cv2.putText(final_img, "RT-DETR Multi-Modal Detection Results", 
                    (combined_width // 2 - 200, 35),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        return final_img